{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e03e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, pandas as pd, torch, torch.nn as nn, joblib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfbd6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_COL   = \"TIMESTAMP\"\n",
    "TARGET_COL = \"TARGETVAR\"\n",
    "BASE_FEATS = [\"U10\",\"V10\",\"U100\",\"V100\"]\n",
    "LAGS_Y     = [1,3,6,12,24]\n",
    "LAGS_SPEED = [1,3,6]\n",
    "ROLLS_Y    = [6,12,24]\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def add_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"speed10\"]  = np.sqrt(out[\"U10\"]**2  + out[\"V10\"]**2)\n",
    "    out[\"speed100\"] = np.sqrt(out[\"U100\"]**2 + out[\"V100\"]**2)\n",
    "    d10  = np.arctan2(out[\"V10\"],  out[\"U10\"])\n",
    "    d100 = np.arctan2(out[\"V100\"], out[\"U100\"])\n",
    "    out[\"dir10_sin\"], out[\"dir10_cos\"]   = np.sin(d10),  np.cos(d10)\n",
    "    out[\"dir100_sin\"], out[\"dir100_cos\"] = np.sin(d100), np.cos(d100)\n",
    "    out[\"shear_speed\"] = out[\"speed100\"] - out[\"speed10\"]\n",
    "    veer = d100 - d10\n",
    "    out[\"veer_sin\"], out[\"veer_cos\"] = np.sin(veer), np.cos(veer)\n",
    "    out[\"hour\"] = pd.to_datetime(out[TIME_COL]).dt.hour\n",
    "    out[\"day\"]  = pd.to_datetime(out[TIME_COL]).dt.dayofyear\n",
    "    out[\"hour_sin\"] = np.sin(2*np.pi*out[\"hour\"]/24.0)\n",
    "    out[\"hour_cos\"] = np.cos(2*np.pi*out[\"hour\"]/24.0)\n",
    "    out[\"day_sin\"]  = np.sin(2*np.pi*out[\"day\"]/366.0)\n",
    "    out[\"day_cos\"]  = np.cos(2*np.pi*out[\"day\"]/366.0)\n",
    "    for L in LAGS_Y:\n",
    "        out[f\"y_lag{L}\"] = out[TARGET_COL].shift(L)\n",
    "    for W in ROLLS_Y:\n",
    "        out[f\"y_roll{W}\"] = out[TARGET_COL].shift(1).rolling(W, min_periods=W).mean()\n",
    "    for L in LAGS_SPEED:\n",
    "        out[f\"speed10_lag{L}\"]  = out[\"speed10\"].shift(L)\n",
    "        out[f\"speed100_lag{L}\"] = out[\"speed100\"].shift(L)\n",
    "    return out\n",
    "\n",
    "def build_feat_list():\n",
    "    return (\n",
    "        BASE_FEATS +\n",
    "        [\"speed10\",\"speed100\",\"dir10_sin\",\"dir10_cos\",\"dir100_sin\",\"dir100_cos\",\n",
    "         \"shear_speed\",\"veer_sin\",\"veer_cos\",\"hour_sin\",\"hour_cos\",\"day_sin\",\"day_cos\"] +\n",
    "        [f\"y_lag{L}\" for L in LAGS_Y] +\n",
    "        [f\"y_roll{W}\" for W in ROLLS_Y] +\n",
    "        [f\"speed10_lag{L}\" for L in LAGS_SPEED] +\n",
    "        [f\"speed100_lag{L}\" for L in LAGS_SPEED]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaab6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "            batch_first=True, dropout=dropout if num_layers>1 else 0.0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        out_size = hidden_size * (2 if bidirectional else 1)\n",
    "        self.norm = nn.LayerNorm(out_size)\n",
    "        self.head = nn.Sequential(nn.Linear(out_size, out_size), nn.GELU(), nn.Dropout(dropout), nn.Linear(out_size,1))\n",
    "    def forward(self, x):\n",
    "        o,_ = self.lstm(x); last = self.norm(o[:, -1, :]); return self.head(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac309d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artifacts(model_root: str):\n",
    "    root = Path(model_root)\n",
    "    with open(root / \"../biLSTM/best_params.json\",\"r\") as f: best_params = json.load(f)\n",
    "    xsc = joblib.load(root / \"../biLSTM/x_scaler_optuna.pkl\")\n",
    "    ysc = joblib.load(root / \"../biLSTM/y_scaler_optuna.pkl\")\n",
    "    state = torch.load(root / \"../biLSTM/bilstm_optuna_best.pt\", map_location=DEVICE)\n",
    "    return best_params, xsc, ysc, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3b945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_from_last24(model_root: str, last24_df: pd.DataFrame, future_weather: dict|None=None):\n",
    "    \"\"\"\n",
    "    last24_df: DataFrame with at least 24 recent rows and columns:\n",
    "        TIMESTAMP, TARGETVAR, U10, V10, U100, V100 (same units/schema as training).\n",
    "    future_weather (optional): dict with keys 'U10','V10','U100','V100' for the *next* hour.\n",
    "        If provided, we’ll use these exogenous values for the t+1 features.\n",
    "        If None, we 'hold' the last known exogenous values.\n",
    "    \"\"\"\n",
    "    best, xsc, ysc, state = load_artifacts(model_root)\n",
    "\n",
    "    df = last24_df.copy().sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "    # If the model’s lookback > provided rows, we can’t predict\n",
    "    lookback = int(best[\"lookback\"])\n",
    "    if len(df) < lookback:\n",
    "        raise ValueError(f\"Need at least {lookback} rows of history; got {len(df)}.\")\n",
    "\n",
    "    # Optionally append a synthetic next-hour exogenous row (same timestamp +1h)\n",
    "    if future_weather is None:\n",
    "        fut = df.iloc[[-1]][[TIME_COL]+BASE_FEATS].copy()\n",
    "        fut[TIME_COL] = pd.to_datetime(fut[TIME_COL]) + pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        fut = pd.DataFrame([{\n",
    "            TIME_COL: pd.to_datetime(df[TIME_COL].iloc[-1]) + pd.Timedelta(hours=1),\n",
    "            \"U10\": future_weather[\"U10\"], \"V10\": future_weather[\"V10\"],\n",
    "            \"U100\": future_weather[\"U100\"], \"V100\": future_weather[\"V100\"],\n",
    "        }])\n",
    "    # set TARGETVAR for the future row temporarily with NaN; we’ll fill it using lags after FE\n",
    "    fut[TARGET_COL] = np.nan\n",
    "\n",
    "    # Build a small working frame = history + placeholder next hour\n",
    "    work = pd.concat([df[[TIME_COL, TARGET_COL]+BASE_FEATS], fut], ignore_index=True)\n",
    "\n",
    "    # For engineered y-lag/roll features, we need actual history TARGETVAR (available in df).\n",
    "    # After FE, the last row will have all lags computed from history; TARGETVAR itself is NaN for that last row.\n",
    "    dfe = add_engineered_features(work)\n",
    "\n",
    "    # Drop only rows that are still incomplete *before* the last row\n",
    "    dfe_hist = dfe.iloc[:-1].dropna().copy()\n",
    "    if len(dfe_hist) < lookback:\n",
    "        raise ValueError(f\"After feature lags/rolls, not enough rows to form a {lookback}-step window. \"\n",
    "                         f\"Provide a bit more history (≥ {lookback+1} rows).\")\n",
    "\n",
    "    # The final feature row we’ll predict on is the very last row (t+1), which has complete lags from history\n",
    "    feat_cols = build_feat_list()\n",
    "    X_hist = dfe_hist[feat_cols].to_numpy(np.float32)\n",
    "\n",
    "    # scale with training scalers\n",
    "    X_hist_s = xsc.transform(X_hist)\n",
    "\n",
    "    # Build the input window (last `lookback` rows)\n",
    "    X_window = X_hist_s[-lookback:, :]                       # shape (lookback, n_feats)\n",
    "    xb = torch.from_numpy(X_window[None, ...]).float().to(DEVICE)\n",
    "\n",
    "    # Rebuild model & load weights\n",
    "    model = BiLSTMRegressor(\n",
    "        input_size=X_window.shape[-1],\n",
    "        hidden_size=int(best[\"hidden\"]),\n",
    "        num_layers=int(best[\"layers\"]),\n",
    "        dropout=float(best[\"dropout\"]),\n",
    "        bidirectional=bool(best[\"bidir\"])\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yhat_s = model(xb).cpu().numpy()                    # scaled\n",
    "    # Invert scaling (and log if used)\n",
    "    yhat = ysc.inverse_transform(yhat_s).ravel()[0]\n",
    "    if bool(best[\"log_target\"]):\n",
    "        yhat = np.expm1(yhat)\n",
    "\n",
    "    next_ts = pd.to_datetime(df[TIME_COL].iloc[-1]) + pd.Timedelta(hours=1)\n",
    "    return next_ts, float(yhat)\n",
    "\n",
    "def _rebuild_model(input_size: int, best_params: dict) -> BiLSTMRegressor:\n",
    "    model = BiLSTMRegressor(\n",
    "        input_size=input_size,\n",
    "        hidden_size=int(best_params[\"hidden\"]),\n",
    "        num_layers=int(best_params[\"layers\"]),\n",
    "        dropout=float(best_params[\"dropout\"]),\n",
    "        bidirectional=bool(best_params[\"bidir\"])\n",
    "    ).to(DEVICE)\n",
    "    return model\n",
    "\n",
    "def predict_over_file(model_root: str, data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run prediction over a whole dataset (same schema as training) and return a DataFrame with:\n",
    "      TIMESTAMP, y_true, y_pred\n",
    "    Notes:\n",
    "      - Uses saved x/y scalers and best_params (lookback, log_target, etc.)\n",
    "      - Aligns outputs after 'lookback' valid rows post-FE\n",
    "    \"\"\"\n",
    "    # load artifacts\n",
    "    best, xsc, ysc, state = load_artifacts(model_root)\n",
    "    lookback = int(best[\"lookback\"])\n",
    "    log_tgt  = bool(best[\"log_target\"])\n",
    "\n",
    "    # load data\n",
    "    if data_path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df = pd.read_excel(data_path)\n",
    "    else:\n",
    "        df = pd.read_csv(data_path)\n",
    "    df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\", infer_datetime_format=True)\n",
    "    df = df.sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "    # FE and target transform (for scaler shape only — true target kept separately)\n",
    "    dfe = add_engineered_features(df).dropna().reset_index(drop=True)\n",
    "    feat_cols = build_feat_list()\n",
    "    X_all = dfe[feat_cols].to_numpy(np.float32)\n",
    "\n",
    "    # make y_true in original space (no scaler), but also scaled for inference inversion\n",
    "    y_true_orig = dfe[TARGET_COL].to_numpy(np.float32).reshape(-1, 1)\n",
    "    y_for_scaler = np.log1p(np.clip(y_true_orig, a_min=0, a_max=None)) if log_tgt else y_true_orig\n",
    "\n",
    "    # scale features/targets\n",
    "    Xs = xsc.transform(X_all)\n",
    "    ys = ysc.transform(y_for_scaler)\n",
    "\n",
    "    # make rolling sequences (lookback windows) and aligned y\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(lookback, len(Xs)):\n",
    "        X_seq.append(Xs[i - lookback:i, :])\n",
    "        y_seq.append(ys[i, 0])\n",
    "    if len(X_seq) == 0:\n",
    "        raise ValueError(f\"Not enough rows after FE to form any lookback={lookback} window; \"\n",
    "                         f\"provide more history.\")\n",
    "    X_seq = np.array(X_seq, np.float32)         # (N, T, F)\n",
    "    y_seq = np.array(y_seq,  np.float32).reshape(-1, 1)\n",
    "\n",
    "    # rebuild + load weights\n",
    "    model = _rebuild_model(X_seq.shape[-1], best)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    # infer in mini-batches\n",
    "    preds_s = []\n",
    "    bs = int(best.get(\"batch\", 128))\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_seq), bs):\n",
    "            xb = torch.from_numpy(X_seq[i:i+bs]).float().to(DEVICE)\n",
    "            pr = model(xb).cpu().numpy()\n",
    "            preds_s.append(pr)\n",
    "    preds_s = np.vstack(preds_s)\n",
    "\n",
    "    # invert target scaling (+ log if used)\n",
    "    y_pred = ysc.inverse_transform(preds_s).ravel()\n",
    "    y_true_scaled = y_seq\n",
    "    y_true_inv = ysc.inverse_transform(y_true_scaled).ravel()\n",
    "\n",
    "    if log_tgt:\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_true_inv = np.expm1(y_true_inv)\n",
    "\n",
    "    # align timestamps (drop the first 'lookback' FE rows)\n",
    "    out_idx = dfe.index[lookback:]\n",
    "    out = pd.DataFrame({\n",
    "        TIME_COL: dfe.loc[out_idx, TIME_COL].values,\n",
    "        \"y_true\": y_true_inv,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "    return out\n",
    "\n",
    "\n",
    "def forecast_multi_steps(model_root: str,\n",
    "                         df_history: pd.DataFrame,\n",
    "                         H: int,\n",
    "                         future_weather_seq: pd.DataFrame | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Recursive H-step forecast from the end of df_history.\n",
    "    Inputs:\n",
    "      - df_history: last N rows with columns [TIMESTAMP, TARGETVAR, U10, V10, U100, V100]\n",
    "      - future_weather_seq: optional DataFrame with H rows and columns [U10,V10,U100,V100].\n",
    "        If None, will 'hold' the last known exogenous features each step.\n",
    "    Output:\n",
    "      DataFrame with TIMESTAMP (t+1 ... t+H) and y_forecast (in original units).\n",
    "    \"\"\"\n",
    "    best, xsc, ysc, state = load_artifacts(model_root)\n",
    "    lookback = int(best[\"lookback\"])\n",
    "    log_tgt  = bool(best[\"log_target\"])\n",
    "\n",
    "    # we’ll reuse your single-step pipeline each step, updating history with predicted TARGETVAR\n",
    "    # build model once (we’ll still call FE each step to update lags/rolls)\n",
    "    # we need input_size; derive it from a one-shot prep using existing history\n",
    "\n",
    "    # small inner helper to get one-step prediction given (history, optional next exogenous)\n",
    "    def _one_step(history_df: pd.DataFrame, fw: dict | None):\n",
    "        # create future placeholder row + FE\n",
    "        df = history_df.copy().sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "        # build future exogenous row\n",
    "        if fw is None:\n",
    "            fut = df.iloc[[-1]][[TIME_COL] + BASE_FEATS].copy()\n",
    "            fut[TIME_COL] = pd.to_datetime(fut[TIME_COL]) + pd.Timedelta(hours=1)\n",
    "        else:\n",
    "            fut = pd.DataFrame([{\n",
    "                TIME_COL: pd.to_datetime(df[TIME_COL].iloc[-1]) + pd.Timedelta(hours=1),\n",
    "                \"U10\": fw[\"U10\"], \"V10\": fw[\"V10\"], \"U100\": fw[\"U100\"], \"V100\": fw[\"V100\"]\n",
    "            }])\n",
    "        fut[TARGET_COL] = np.nan\n",
    "\n",
    "        work = pd.concat([df[[TIME_COL, TARGET_COL] + BASE_FEATS], fut], ignore_index=True)\n",
    "\n",
    "        dfe = add_engineered_features(work)\n",
    "        dfe_hist = dfe.iloc[:-1].dropna().copy()\n",
    "        if len(dfe_hist) < lookback:\n",
    "            # compute minimum rows needed considering max lag/roll (=24 here)\n",
    "            req = 24 + lookback\n",
    "            raise ValueError(f\"Not enough rows after FE to form lookback={lookback} window. \"\n",
    "                             f\"Provide ≥ {req} rows of raw history (have {len(history_df)}).\")\n",
    "\n",
    "        feat_cols = build_feat_list()\n",
    "        X_hist = dfe_hist[feat_cols].to_numpy(np.float32)\n",
    "        X_hist_s = xsc.transform(X_hist)\n",
    "        X_window = X_hist_s[-lookback:, :]\n",
    "        xb = torch.from_numpy(X_window[None, ...]).float().to(DEVICE)\n",
    "\n",
    "        return xb, pd.to_datetime(df[TIME_COL].iloc[-1]) + pd.Timedelta(hours=1)\n",
    "\n",
    "    # initialize model lazily on first step (to know input_size)\n",
    "    fw0 = None\n",
    "    if future_weather_seq is not None and len(future_weather_seq) > 0:\n",
    "        row0 = future_weather_seq.iloc[0]\n",
    "        fw0 = {\"U10\": float(row0[\"U10\"]), \"V10\": float(row0[\"V10\"]),\n",
    "               \"U100\": float(row0[\"U100\"]), \"V100\": float(row0[\"V100\"])}\n",
    "    xb0, _ = _one_step(df_history, fw0)\n",
    "    model = _rebuild_model(xb0.shape[-1], best)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    preds, times = [], []\n",
    "    cur_hist = df_history.copy()\n",
    "\n",
    "    for h in range(H):\n",
    "        fw = None\n",
    "        if future_weather_seq is not None:\n",
    "            row = future_weather_seq.iloc[h]\n",
    "            fw = {\"U10\": float(row[\"U10\"]), \"V10\": float(row[\"V10\"]),\n",
    "                  \"U100\": float(row[\"U100\"]), \"V100\": float(row[\"V100\"])}\n",
    "\n",
    "        xb, t_next = _one_step(cur_hist, fw)\n",
    "        with torch.no_grad():\n",
    "            yhat_s = model(xb).cpu().numpy()\n",
    "        yhat = ysc.inverse_transform(yhat_s).ravel()[0]\n",
    "        if log_tgt:\n",
    "            yhat = np.expm1(yhat)\n",
    "\n",
    "        preds.append(float(yhat))\n",
    "        times.append(t_next)\n",
    "\n",
    "        # append predicted step to history so lags/rolls update\n",
    "        add_row = cur_hist.iloc[[-1]][[TIME_COL] + BASE_FEATS].copy()\n",
    "        add_row[TIME_COL] = t_next\n",
    "        add_row[TARGET_COL] = yhat\n",
    "        if fw is not None:\n",
    "            add_row[\"U10\"] = fw[\"U10\"]; add_row[\"V10\"] = fw[\"V10\"]\n",
    "            add_row[\"U100\"] = fw[\"U100\"]; add_row[\"V100\"] = fw[\"V100\"]\n",
    "        cur_hist = pd.concat([cur_hist, add_row], ignore_index=True)\n",
    "\n",
    "    return pd.DataFrame({TIME_COL: times, \"y_forecast\": preds})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec5849ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-01 01:00:00 0.25088977813720703\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Put your last 24 rows into a DataFrame:\n",
    "# Must have columns: TIMESTAMP, TARGETVAR, U10, V10, U100, V100\n",
    "last24 = pd.read_excel(\"../Predictions/WindPowerForecastingData.xlsx\")  # or build manually\n",
    "\n",
    "# 2) If you already know the next hour’s weather, pass it (optional):\n",
    "future_weather = {\n",
    "    \"U10\": 3.5, \"V10\": -1.2,\n",
    "    \"U100\": 5.1, \"V100\": -1.8\n",
    "}\n",
    "# If you don’t know it, set future_weather=None (the code will hold the last known values)\n",
    "\n",
    "# 3) Predict the next hour:\n",
    "next_ts, y_pred = predict_next_from_last24(\n",
    "    model_root=\".\",            # folder with best_params.json + biLSTM/*\n",
    "    last24_df=last24,\n",
    "    future_weather=None        # or future_weather dict as above\n",
    ")\n",
    "print(next_ts, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e19c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus X513\\AppData\\Local\\Temp\\ipykernel_1340\\1303936147.py:179: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            TIMESTAMP    y_true    y_pred\n",
      "0 2012-01-03 01:00:00  0.191617  0.082822\n",
      "1 2012-01-03 02:00:00  0.392726  0.069542\n",
      "2 2012-01-03 03:00:00  0.279485  0.138238\n",
      "3 2012-01-03 04:00:00  0.129217  0.262185\n",
      "4 2012-01-03 05:00:00  0.117658  0.198204\n"
     ]
    }
   ],
   "source": [
    "# 1) Predict across a whole file\n",
    "df_pred = predict_over_file(\n",
    "    model_root=\".\",                             # where ../biLSTM/* is relative to this script\n",
    "    data_path=\"E:/WindPowerForecastingData.xlsx\"  # or .csv\n",
    "               # or None/0 for CSV\n",
    ")\n",
    "print(df_pred.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50fadfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_897/2815044687.py:14: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            TIMESTAMP  y_forecast\n",
      "0 2013-12-01 01:00:00    0.183762\n",
      "1 2013-12-01 02:00:00    0.183762\n",
      "2 2013-12-01 03:00:00    0.183762\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 0) Pick where your recent history comes from\n",
    "# Option A: load a file that ALREADY has your recent rows\n",
    "hist = pd.read_excel(\"../Predictions/WindPowerForecastingData.xlsx\")  # or .csv\n",
    "\n",
    "# Option B: load a full dataset, then slice the last N rows as history\n",
    "# full = pd.read_excel(\"WindPowerForecastingData.xlsx\", sheet_name=\"WindPowerForecastingData\")\n",
    "# hist = full.tail(80).copy()  # example: last 80 rows\n",
    "\n",
    "# 1) Sanitize timestamps & numerics (the helper you added)\n",
    "def sanitize_history(df, time_col=\"TIMESTAMP\"):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\", infer_datetime_format=True)\n",
    "    bad = df[time_col].isna()\n",
    "    if bad.any():\n",
    "        raise ValueError(f\"{bad.sum()} TIMESTAMP values could not be parsed. \"\n",
    "                         f\"Examples: {df.loc[bad, time_col].head(3).tolist()}\")\n",
    "    for col in [\"U10\",\"V10\",\"U100\",\"V100\",\"TARGETVAR\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    return df.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "hist = sanitize_history(hist)\n",
    "\n",
    "# 2) (Recommended) ensure you have enough rows for your model:\n",
    "#    required_rows = max_lag_or_roll(=24, or 48 if turbulence) + lookback\n",
    "# For Optuna pipeline (no turbulence): 24 + lookback\n",
    "# print to be sure:\n",
    "# print(\"Rows in hist:\", len(hist))\n",
    "\n",
    "# 3) Now run the forecast\n",
    "df_fore = forecast_multi_steps(\n",
    "    model_root=\".\",\n",
    "    df_history=hist,\n",
    "    H=3,\n",
    "    future_weather_seq=None  # or a DataFrame with H rows & columns U10,V10,U100,V100\n",
    ")\n",
    "print(df_fore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9259ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus X513\\AppData\\Local\\Temp\\ipykernel_1340\\1303936147.py:179: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "df_pred = predict_over_file(model_root=\".\", data_path=r\"E:\\WindPowerForecastingData.xlsx\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Align arrays (if needed)\n",
    "min_len = min(len(df_pred[\"y_true\"]), len(df_pred[\"y_pred\"]))\n",
    "y_true_aligned = np.asarray(df_pred[\"y_true\"]).reshape(-1)[:min_len]\n",
    "y_pred_aligned = np.asarray(df_pred[\"y_pred\"]).reshape(-1)[:min_len]\n",
    "\n",
    "# Save to CSV\n",
    "forecast_df = pd.DataFrame({\n",
    "    \"TIMESTAMP\": df_pred[\"TIMESTAMP\"].values[:min_len],\n",
    "    \"Actual\": y_true_aligned,\n",
    "    \"Predicted\": y_pred_aligned\n",
    "})\n",
    "forecast_df.to_csv(\"forecast_results_bilstm.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e7250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.0766\n",
      "Mean Squared Error (MSE): 0.0132\n",
      "Root Mean Squared Error (RMSE): 0.1148\n",
      "R² Score: 0.8426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Extract actual and predicted values\n",
    "y_true = df_pred[\"y_true\"].values\n",
    "y_pred = df_pred[\"y_pred\"].values\n",
    "\n",
    "# Calculate error metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
