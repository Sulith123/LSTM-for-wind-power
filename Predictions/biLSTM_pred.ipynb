{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e03e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, pandas as pd, torch, torch.nn as nn, joblib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_COL   = \"TIMESTAMP\"\n",
    "TARGET_COL = \"TARGETVAR\"\n",
    "BASE_FEATS = [\"U10\",\"V10\",\"U100\",\"V100\"]\n",
    "LAGS_Y     = [1,3,6,12,24]\n",
    "LAGS_SPEED = [1,3,6]\n",
    "ROLLS_Y    = [6,12,24]\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def add_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"speed10\"]  = np.sqrt(out[\"U10\"]**2  + out[\"V10\"]**2)\n",
    "    out[\"speed100\"] = np.sqrt(out[\"U100\"]**2 + out[\"V100\"]**2)\n",
    "    d10  = np.arctan2(out[\"V10\"],  out[\"U10\"])\n",
    "    d100 = np.arctan2(out[\"V100\"], out[\"U100\"])\n",
    "    out[\"dir10_sin\"], out[\"dir10_cos\"]   = np.sin(d10),  np.cos(d10)\n",
    "    out[\"dir100_sin\"], out[\"dir100_cos\"] = np.sin(d100), np.cos(d100)\n",
    "    out[\"shear_speed\"] = out[\"speed100\"] - out[\"speed10\"]\n",
    "    veer = d100 - d10\n",
    "    out[\"veer_sin\"], out[\"veer_cos\"] = np.sin(veer), np.cos(veer)\n",
    "    out[\"hour\"] = pd.to_datetime(out[TIME_COL]).dt.hour\n",
    "    out[\"day\"]  = pd.to_datetime(out[TIME_COL]).dt.dayofyear\n",
    "    out[\"hour_sin\"] = np.sin(2*np.pi*out[\"hour\"]/24.0)\n",
    "    out[\"hour_cos\"] = np.cos(2*np.pi*out[\"hour\"]/24.0)\n",
    "    out[\"day_sin\"]  = np.sin(2*np.pi*out[\"day\"]/366.0)\n",
    "    out[\"day_cos\"]  = np.cos(2*np.pi*out[\"day\"]/366.0)\n",
    "    for L in LAGS_Y:\n",
    "        out[f\"y_lag{L}\"] = out[TARGET_COL].shift(L)\n",
    "    for W in ROLLS_Y:\n",
    "        out[f\"y_roll{W}\"] = out[TARGET_COL].shift(1).rolling(W, min_periods=W).mean()\n",
    "    for L in LAGS_SPEED:\n",
    "        out[f\"speed10_lag{L}\"]  = out[\"speed10\"].shift(L)\n",
    "        out[f\"speed100_lag{L}\"] = out[\"speed100\"].shift(L)\n",
    "    return out\n",
    "\n",
    "def build_feat_list():\n",
    "    return (\n",
    "        BASE_FEATS +\n",
    "        [\"speed10\",\"speed100\",\"dir10_sin\",\"dir10_cos\",\"dir100_sin\",\"dir100_cos\",\n",
    "         \"shear_speed\",\"veer_sin\",\"veer_cos\",\"hour_sin\",\"hour_cos\",\"day_sin\",\"day_cos\"] +\n",
    "        [f\"y_lag{L}\" for L in LAGS_Y] +\n",
    "        [f\"y_roll{W}\" for W in ROLLS_Y] +\n",
    "        [f\"speed10_lag{L}\" for L in LAGS_SPEED] +\n",
    "        [f\"speed100_lag{L}\" for L in LAGS_SPEED]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "            batch_first=True, dropout=dropout if num_layers>1 else 0.0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        out_size = hidden_size * (2 if bidirectional else 1)\n",
    "        self.norm = nn.LayerNorm(out_size)\n",
    "        self.head = nn.Sequential(nn.Linear(out_size, out_size), nn.GELU(), nn.Dropout(dropout), nn.Linear(out_size,1))\n",
    "    def forward(self, x):\n",
    "        o,_ = self.lstm(x); last = self.norm(o[:, -1, :]); return self.head(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac309d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artifacts(model_root: str):\n",
    "    root = Path(model_root)\n",
    "    with open(root / \"../biLSTM/best_params.json\",\"r\") as f: best_params = json.load(f)\n",
    "    xsc = joblib.load(root / \"../biLSTM/x_scaler_optuna.pkl\")\n",
    "    ysc = joblib.load(root / \"../biLSTM/y_scaler_optuna.pkl\")\n",
    "    state = torch.load(root / \"../biLSTM/bilstm_optuna_best.pt\", map_location=DEVICE)\n",
    "    return best_params, xsc, ysc, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_from_last24(model_root: str, last24_df: pd.DataFrame, future_weather: dict|None=None):\n",
    "    \"\"\"\n",
    "    last24_df: DataFrame with at least 24 recent rows and columns:\n",
    "        TIMESTAMP, TARGETVAR, U10, V10, U100, V100 (same units/schema as training).\n",
    "    future_weather (optional): dict with keys 'U10','V10','U100','V100' for the *next* hour.\n",
    "        If provided, we’ll use these exogenous values for the t+1 features.\n",
    "        If None, we 'hold' the last known exogenous values.\n",
    "    \"\"\"\n",
    "    best, xsc, ysc, state = load_artifacts(model_root)\n",
    "\n",
    "    df = last24_df.copy().sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "    # If the model’s lookback > provided rows, we can’t predict\n",
    "    lookback = int(best[\"lookback\"])\n",
    "    if len(df) < lookback:\n",
    "        raise ValueError(f\"Need at least {lookback} rows of history; got {len(df)}.\")\n",
    "\n",
    "    # Optionally append a synthetic next-hour exogenous row (same timestamp +1h)\n",
    "    if future_weather is None:\n",
    "        fut = df.iloc[[-1]][[TIME_COL]+BASE_FEATS].copy()\n",
    "        fut[TIME_COL] = pd.to_datetime(fut[TIME_COL]) + pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        fut = pd.DataFrame([{\n",
    "            TIME_COL: pd.to_datetime(df[TIME_COL].iloc[-1]) + pd.Timedelta(hours=1),\n",
    "            \"U10\": future_weather[\"U10\"], \"V10\": future_weather[\"V10\"],\n",
    "            \"U100\": future_weather[\"U100\"], \"V100\": future_weather[\"V100\"],\n",
    "        }])\n",
    "    # set TARGETVAR for the future row temporarily with NaN; we’ll fill it using lags after FE\n",
    "    fut[TARGET_COL] = np.nan\n",
    "\n",
    "    # Build a small working frame = history + placeholder next hour\n",
    "    work = pd.concat([df[[TIME_COL, TARGET_COL]+BASE_FEATS], fut], ignore_index=True)\n",
    "\n",
    "    # For engineered y-lag/roll features, we need actual history TARGETVAR (available in df).\n",
    "    # After FE, the last row will have all lags computed from history; TARGETVAR itself is NaN for that last row.\n",
    "    dfe = add_engineered_features(work)\n",
    "\n",
    "    # Drop only rows that are still incomplete *before* the last row\n",
    "    dfe_hist = dfe.iloc[:-1].dropna().copy()\n",
    "    if len(dfe_hist) < lookback:\n",
    "        raise ValueError(f\"After feature lags/rolls, not enough rows to form a {lookback}-step window. \"\n",
    "                         f\"Provide a bit more history (≥ {lookback+1} rows).\")\n",
    "\n",
    "    # The final feature row we’ll predict on is the very last row (t+1), which has complete lags from history\n",
    "    feat_cols = build_feat_list()\n",
    "    X_hist = dfe_hist[feat_cols].to_numpy(np.float32)\n",
    "\n",
    "    # scale with training scalers\n",
    "    X_hist_s = xsc.transform(X_hist)\n",
    "\n",
    "    # Build the input window (last `lookback` rows)\n",
    "    X_window = X_hist_s[-lookback:, :]                       # shape (lookback, n_feats)\n",
    "    xb = torch.from_numpy(X_window[None, ...]).float().to(DEVICE)\n",
    "\n",
    "    # Rebuild model & load weights\n",
    "    model = BiLSTMRegressor(\n",
    "        input_size=X_window.shape[-1],\n",
    "        hidden_size=int(best[\"hidden\"]),\n",
    "        num_layers=int(best[\"layers\"]),\n",
    "        dropout=float(best[\"dropout\"]),\n",
    "        bidirectional=bool(best[\"bidir\"])\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yhat_s = model(xb).cpu().numpy()                    # scaled\n",
    "    # Invert scaling (and log if used)\n",
    "    yhat = ysc.inverse_transform(yhat_s).ravel()[0]\n",
    "    if bool(best[\"log_target\"]):\n",
    "        yhat = np.expm1(yhat)\n",
    "\n",
    "    next_ts = pd.to_datetime(df[TIME_COL].iloc[-1]) + pd.Timedelta(hours=1)\n",
    "    return next_ts, float(yhat)\n",
    "\n",
    "def _rebuild_model(input_size: int, best_params: dict) -> BiLSTMRegressor:\n",
    "    model = BiLSTMRegressor(\n",
    "        input_size=input_size,\n",
    "        hidden_size=int(best_params[\"hidden\"]),\n",
    "        num_layers=int(best_params[\"layers\"]),\n",
    "        dropout=float(best_params[\"dropout\"]),\n",
    "        bidirectional=bool(best_params[\"bidir\"])\n",
    "    ).to(DEVICE)\n",
    "    return model\n",
    "\n",
    "def predict_over_file(model_root: str, data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run prediction over a whole dataset (same schema as training) and return a DataFrame with:\n",
    "      TIMESTAMP, y_true, y_pred\n",
    "    Notes:\n",
    "      - Uses saved x/y scalers and best_params (lookback, log_target, etc.)\n",
    "      - Aligns outputs after 'lookback' valid rows post-FE\n",
    "    \"\"\"\n",
    "    # load artifacts\n",
    "    best, xsc, ysc, state = load_artifacts(model_root)\n",
    "    lookback = int(best[\"lookback\"])\n",
    "    log_tgt  = bool(best[\"log_target\"])\n",
    "\n",
    "    # load data\n",
    "    if data_path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df = pd.read_excel(data_path)\n",
    "    else:\n",
    "        df = pd.read_csv(data_path)\n",
    "    df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\", infer_datetime_format=True)\n",
    "    df = df.sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "    # FE and target transform (for scaler shape only — true target kept separately)\n",
    "    dfe = add_engineered_features(df).dropna().reset_index(drop=True)\n",
    "    feat_cols = build_feat_list()\n",
    "    X_all = dfe[feat_cols].to_numpy(np.float32)\n",
    "\n",
    "    # make y_true in original space (no scaler), but also scaled for inference inversion\n",
    "    y_true_orig = dfe[TARGET_COL].to_numpy(np.float32).reshape(-1, 1)\n",
    "    y_for_scaler = np.log1p(np.clip(y_true_orig, a_min=0, a_max=None)) if log_tgt else y_true_orig\n",
    "\n",
    "    # scale features/targets\n",
    "    Xs = xsc.transform(X_all)\n",
    "    ys = ysc.transform(y_for_scaler)\n",
    "\n",
    "    # make rolling sequences (lookback windows) and aligned y\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(lookback, len(Xs)):\n",
    "        X_seq.append(Xs[i - lookback:i, :])\n",
    "        y_seq.append(ys[i, 0])\n",
    "    if len(X_seq) == 0:\n",
    "        raise ValueError(f\"Not enough rows after FE to form any lookback={lookback} window; \"\n",
    "                         f\"provide more history.\")\n",
    "    X_seq = np.array(X_seq, np.float32)         # (N, T, F)\n",
    "    y_seq = np.array(y_seq,  np.float32).reshape(-1, 1)\n",
    "\n",
    "    # rebuild + load weights\n",
    "    model = _rebuild_model(X_seq.shape[-1], best)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    # infer in mini-batches\n",
    "    preds_s = []\n",
    "    bs = int(best.get(\"batch\", 128))\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_seq), bs):\n",
    "            xb = torch.from_numpy(X_seq[i:i+bs]).float().to(DEVICE)\n",
    "            pr = model(xb).cpu().numpy()\n",
    "            preds_s.append(pr)\n",
    "    preds_s = np.vstack(preds_s)\n",
    "\n",
    "    # invert target scaling (+ log if used)\n",
    "    y_pred = ysc.inverse_transform(preds_s).ravel()\n",
    "    y_true_scaled = y_seq\n",
    "    y_true_inv = ysc.inverse_transform(y_true_scaled).ravel()\n",
    "\n",
    "    if log_tgt:\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_true_inv = np.expm1(y_true_inv)\n",
    "\n",
    "    # align timestamps (drop the first 'lookback' FE rows)\n",
    "    out_idx = dfe.index[lookback:]\n",
    "    out = pd.DataFrame({\n",
    "        TIME_COL: dfe.loc[out_idx, TIME_COL].values,\n",
    "        \"y_true\": y_true_inv,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n",
    "    return out\n",
    "\n",
    "\n",
    "def forecast_multi_steps(model_root: str,\n",
    "                         df_history: pd.DataFrame,\n",
    "                         H: int,\n",
    "                         future_weather_seq: pd.DataFrame | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Recursive H-step forecast from the end of df_history.\n",
    "    Inputs:\n",
    "      - df_history: last N rows with columns [TIMESTAMP, TARGETVAR, U10, V10, U100, V100]\n",
    "      - future_weather_seq: optional DataFrame with H rows and columns [U10,V10,U100,V100].\n",
    "        If None, will 'hold' the last known exogenous features each step.\n",
    "    Output:\n",
    "      DataFrame with TIMESTAMP (t+1 ... t+H) and y_forecast (in original units).\n",
    "    \"\"\"\n",
    "    best, xsc, ysc, state = load_artifacts(model_root)\n",
    "    lookback = int(best[\"lookback\"])\n",
    "    log_tgt  = bool(best[\"log_target\"])\n",
    "\n",
    "    # we’ll reuse your single-step pipeline each step, updating history with predicted TARGETVAR\n",
    "    # build model once (we’ll still call FE each step to update lags/rolls)\n",
    "    # we need input_size; derive it from a one-shot prep using existing history\n",
    "\n",
    "    # small inner helper to get one-step prediction given (history, optional next exogenous)\n",
    "    def _one_step(history_df: pd.DataFrame, fw: dict | None):\n",
    "        # create future placeholder row + FE\n",
    "        df = history_df.copy().sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "        # build future exogenous row\n",
    "        if fw is None:\n",
    "            fut = df.iloc[[-1]][[TIME_COL] + BASE_FEATS].copy()\n",
    "            fut[TIME_COL] = pd.to_datetime(fut[TIME_COL]) + pd.Timedelta(hours=1)\n",
    "        else:\n",
    "            fut = pd.DataFrame([{\n",
    "                TIME_COL: pd.to_datetime(df[TIME_COL].iloc[-1]) + pd.Timedelta(hours=1),\n",
    "                \"U10\": fw[\"U10\"], \"V10\": fw[\"V10\"], \"U100\": fw[\"U100\"], \"V100\": fw[\"V100\"]\n",
    "            }])\n",
    "        fut[TARGET_COL] = np.nan\n",
    "\n",
    "        work = pd.concat([df[[TIME_COL, TARGET_COL] + BASE_FEATS], fut], ignore_index=True)\n",
    "\n",
    "        dfe = add_engineered_features(work)\n",
    "        dfe_hist = dfe.iloc[:-1].dropna().copy()\n",
    "        if len(dfe_hist) < lookback:\n",
    "            # compute minimum rows needed considering max lag/roll (=24 here)\n",
    "            req = 24 + lookback\n",
    "            raise ValueError(f\"Not enough rows after FE to form lookback={lookback} window. \"\n",
    "                             f\"Provide ≥ {req} rows of raw history (have {len(history_df)}).\")\n",
    "\n",
    "        feat_cols = build_feat_list()\n",
    "        X_hist = dfe_hist[feat_cols].to_numpy(np.float32)\n",
    "        X_hist_s = xsc.transform(X_hist)\n",
    "        X_window = X_hist_s[-lookback:, :]\n",
    "        xb = torch.from_numpy(X_window[None, ...]).float().to(DEVICE)\n",
    "\n",
    "        return xb, pd.to_datetime(df[TIME_COL].iloc[-1]) + pd.Timedelta(hours=1)\n",
    "\n",
    "    # initialize model lazily on first step (to know input_size)\n",
    "    fw0 = None\n",
    "    if future_weather_seq is not None and len(future_weather_seq) > 0:\n",
    "        row0 = future_weather_seq.iloc[0]\n",
    "        fw0 = {\"U10\": float(row0[\"U10\"]), \"V10\": float(row0[\"V10\"]),\n",
    "               \"U100\": float(row0[\"U100\"]), \"V100\": float(row0[\"V100\"])}\n",
    "    xb0, _ = _one_step(df_history, fw0)\n",
    "    model = _rebuild_model(xb0.shape[-1], best)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    preds, times = [], []\n",
    "    cur_hist = df_history.copy()\n",
    "\n",
    "    for h in range(H):\n",
    "        fw = None\n",
    "        if future_weather_seq is not None:\n",
    "            row = future_weather_seq.iloc[h]\n",
    "            fw = {\"U10\": float(row[\"U10\"]), \"V10\": float(row[\"V10\"]),\n",
    "                  \"U100\": float(row[\"U100\"]), \"V100\": float(row[\"V100\"])}\n",
    "\n",
    "        xb, t_next = _one_step(cur_hist, fw)\n",
    "        with torch.no_grad():\n",
    "            yhat_s = model(xb).cpu().numpy()\n",
    "        yhat = ysc.inverse_transform(yhat_s).ravel()[0]\n",
    "        if log_tgt:\n",
    "            yhat = np.expm1(yhat)\n",
    "\n",
    "        preds.append(float(yhat))\n",
    "        times.append(t_next)\n",
    "\n",
    "        # append predicted step to history so lags/rolls update\n",
    "        add_row = cur_hist.iloc[[-1]][[TIME_COL] + BASE_FEATS].copy()\n",
    "        add_row[TIME_COL] = t_next\n",
    "        add_row[TARGET_COL] = yhat\n",
    "        if fw is not None:\n",
    "            add_row[\"U10\"] = fw[\"U10\"]; add_row[\"V10\"] = fw[\"V10\"]\n",
    "            add_row[\"U100\"] = fw[\"U100\"]; add_row[\"V100\"] = fw[\"V100\"]\n",
    "        cur_hist = pd.concat([cur_hist, add_row], ignore_index=True)\n",
    "\n",
    "    return pd.DataFrame({TIME_COL: times, \"y_forecast\": preds})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5849ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After feature lags/rolls, not enough rows to form a 24-step window. Provide a bit more history (≥ 25 rows).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      8\u001b[39m future_weather = {\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mU10\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3.5\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mV10\u001b[39m\u001b[33m\"\u001b[39m: -\u001b[32m1.2\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mU100\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m5.1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mV100\u001b[39m\u001b[33m\"\u001b[39m: -\u001b[32m1.8\u001b[39m\n\u001b[32m     11\u001b[39m }\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# If you don’t know it, set future_weather=None (the code will hold the last known values)\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 3) Predict the next hour:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m next_ts, y_pred = \u001b[43mpredict_next_from_last24\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_root\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# folder with best_params.json + biLSTM/*\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlast24_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlast24\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_weather\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# or future_weather dict as above\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(next_ts, y_pred)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mpredict_next_from_last24\u001b[39m\u001b[34m(model_root, last24_df, future_weather)\u001b[39m\n\u001b[32m    110\u001b[39m dfe_hist = dfe.iloc[:-\u001b[32m1\u001b[39m].dropna().copy()\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dfe_hist) < lookback:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAfter feature lags/rolls, not enough rows to form a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlookback\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-step window. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProvide a bit more history (≥ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlookback+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# The final feature row we’ll predict on is the very last row (t+1), which has complete lags from history\u001b[39;00m\n\u001b[32m    116\u001b[39m feat_cols = build_feat_list()\n",
      "\u001b[31mValueError\u001b[39m: After feature lags/rolls, not enough rows to form a 24-step window. Provide a bit more history (≥ 25 rows)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Put your last 24 rows into a DataFrame:\n",
    "# Must have columns: TIMESTAMP, TARGETVAR, U10, V10, U100, V100\n",
    "last24 = pd.read_csv(\"my_last_24.csv\")  # or build manually\n",
    "\n",
    "# 2) If you already know the next hour’s weather, pass it (optional):\n",
    "future_weather = {\n",
    "    \"U10\": 3.5, \"V10\": -1.2,\n",
    "    \"U100\": 5.1, \"V100\": -1.8\n",
    "}\n",
    "# If you don’t know it, set future_weather=None (the code will hold the last known values)\n",
    "\n",
    "# 3) Predict the next hour:\n",
    "next_ts, y_pred = predict_next_from_last24(\n",
    "    model_root=\".\",            # folder with best_params.json + biLSTM/*\n",
    "    last24_df=last24,\n",
    "    future_weather=None        # or future_weather dict as above\n",
    ")\n",
    "print(next_ts, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4e19c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus X513\\AppData\\Local\\Temp\\ipykernel_1340\\1303936147.py:179: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            TIMESTAMP    y_true    y_pred\n",
      "0 2012-01-03 01:00:00  0.191617  0.082822\n",
      "1 2012-01-03 02:00:00  0.392726  0.069542\n",
      "2 2012-01-03 03:00:00  0.279485  0.138238\n",
      "3 2012-01-03 04:00:00  0.129217  0.262185\n",
      "4 2012-01-03 05:00:00  0.117658  0.198204\n"
     ]
    }
   ],
   "source": [
    "# 1) Predict across a whole file\n",
    "df_pred = predict_over_file(\n",
    "    model_root=\".\",                             # where ../biLSTM/* is relative to this script\n",
    "    data_path=\"E:/WindPowerForecastingData.xlsx\"  # or .csv\n",
    "               # or None/0 for CSV\n",
    ")\n",
    "print(df_pred.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fadfab",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'my_recent_history.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2) Multi-step forecast from your latest history\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m hist = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmy_recent_history.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# must have TIMESTAMP, TARGETVAR, U10, V10, U100, V100\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# optional exogenous for next H hours:\u001b[39;00m\n\u001b[32m      6\u001b[39m fw = pd.DataFrame([\n\u001b[32m      7\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mU10\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3.0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mV10\u001b[39m\u001b[33m\"\u001b[39m: -\u001b[32m0.7\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mU100\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4.8\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mV100\u001b[39m\u001b[33m\"\u001b[39m: -\u001b[32m1.3\u001b[39m},\n\u001b[32m      8\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mU10\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3.1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mV10\u001b[39m\u001b[33m\"\u001b[39m: -\u001b[32m0.6\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mU100\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4.9\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mV100\u001b[39m\u001b[33m\"\u001b[39m: -\u001b[32m1.2\u001b[39m},\n\u001b[32m      9\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mU10\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3.2\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mV10\u001b[39m\u001b[33m\"\u001b[39m: -\u001b[32m0.5\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mU100\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m5.0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mV100\u001b[39m\u001b[33m\"\u001b[39m: -\u001b[32m1.1\u001b[39m},\n\u001b[32m     10\u001b[39m ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus X513\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus X513\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus X513\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus X513\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'my_recent_history.xlsx'"
     ]
    }
   ],
   "source": [
    "# 2) Multi-step forecast from your latest history\n",
    "import pandas as pd\n",
    "hist = pd.read_excel(\"my_recent_history.xlsx\")   # must have TIMESTAMP, TARGETVAR, U10, V10, U100, V100\n",
    "\n",
    "# optional exogenous for next H hours:\n",
    "fw = pd.DataFrame([\n",
    "    {\"U10\": 3.0, \"V10\": -0.7, \"U100\": 4.8, \"V100\": -1.3},\n",
    "    {\"U10\": 3.1, \"V10\": -0.6, \"U100\": 4.9, \"V100\": -1.2},\n",
    "    {\"U10\": 3.2, \"V10\": -0.5, \"U100\": 5.0, \"V100\": -1.1},\n",
    "])\n",
    "\n",
    "future = forecast_multi_steps(\n",
    "    model_root=\".\",\n",
    "    df_history=hist,\n",
    "    H=3,\n",
    "    future_weather_seq=None  # or fw\n",
    ")\n",
    "print(future)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9259ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus X513\\AppData\\Local\\Temp\\ipykernel_1340\\1303936147.py:179: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "df_pred = predict_over_file(model_root=\".\", data_path=r\"E:\\WindPowerForecastingData.xlsx\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Align arrays (if needed)\n",
    "min_len = min(len(df_pred[\"y_true\"]), len(df_pred[\"y_pred\"]))\n",
    "y_true_aligned = np.asarray(df_pred[\"y_true\"]).reshape(-1)[:min_len]\n",
    "y_pred_aligned = np.asarray(df_pred[\"y_pred\"]).reshape(-1)[:min_len]\n",
    "\n",
    "# Save to CSV\n",
    "forecast_df = pd.DataFrame({\n",
    "    \"TIMESTAMP\": df_pred[\"TIMESTAMP\"].values[:min_len],\n",
    "    \"Actual\": y_true_aligned,\n",
    "    \"Predicted\": y_pred_aligned\n",
    "})\n",
    "forecast_df.to_csv(\"forecast_results_bilstm.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2e7250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.0766\n",
      "Mean Squared Error (MSE): 0.0132\n",
      "Root Mean Squared Error (RMSE): 0.1148\n",
      "R² Score: 0.8426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Extract actual and predicted values\n",
    "y_true = df_pred[\"y_true\"].values\n",
    "y_pred = df_pred[\"y_pred\"].values\n",
    "\n",
    "# Calculate error metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
