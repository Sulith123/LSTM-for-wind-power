{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af96fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ga_predict_helpers.py\n",
    "import json, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F, joblib\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "# ===== Match your training code =====\n",
    "TIME_COL   = \"TIMESTAMP\"\n",
    "TARGET_COL = \"TARGETVAR\"\n",
    "BASE_FEATS = [\"U10\", \"V10\", \"U100\", \"V100\"]\n",
    "\n",
    "LAGS_Y       = [1, 3, 6, 12, 24]\n",
    "LAGS_SPEED   = [1, 3, 6]\n",
    "ROLLS_Y      = [6, 12, 24]\n",
    "TURB_WINS    = [6, 12, 24, 48]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- FE (same as training) ----------\n",
    "def add_engineered_features(df: pd.DataFrame, include_turbulence: bool) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"speed10\"]  = np.sqrt(out[\"U10\"]**2  + out[\"V10\"]**2)\n",
    "    out[\"speed100\"] = np.sqrt(out[\"U100\"]**2 + out[\"V100\"]**2)\n",
    "    d10  = np.arctan2(out[\"V10\"],  out[\"U10\"])\n",
    "    d100 = np.arctan2(out[\"V100\"], out[\"U100\"])\n",
    "    out[\"dir10_sin\"], out[\"dir10_cos\"]   = np.sin(d10),  np.cos(d10)\n",
    "    out[\"dir100_sin\"], out[\"dir100_cos\"] = np.sin(d100), np.cos(d100)\n",
    "    out[\"shear_speed\"] = out[\"speed100\"] - out[\"speed10\"]\n",
    "    veer = d100 - d10\n",
    "    out[\"veer_sin\"], out[\"veer_cos\"] = np.sin(veer), np.cos(veer)\n",
    "\n",
    "    out[\"hour\"] = pd.to_datetime(out[TIME_COL]).dt.hour\n",
    "    out[\"day\"]  = pd.to_datetime(out[TIME_COL]).dt.dayofyear\n",
    "    out[\"hour_sin\"] = np.sin(2*np.pi*out[\"hour\"]/24.0)\n",
    "    out[\"hour_cos\"] = np.cos(2*np.pi*out[\"hour\"]/24.0)\n",
    "    out[\"day_sin\"]  = np.sin(2*np.pi*out[\"day\"]/366.0)\n",
    "    out[\"day_cos\"]  = np.cos(2*np.pi*out[\"day\"]/366.0)\n",
    "\n",
    "    for L in LAGS_Y:\n",
    "        out[f\"y_lag{L}\"] = out[TARGET_COL].shift(L)\n",
    "    for W in ROLLS_Y:\n",
    "        out[f\"y_roll{W}\"] = out[TARGET_COL].shift(1).rolling(W, min_periods=W).mean()\n",
    "    for L in LAGS_SPEED:\n",
    "        out[f\"speed10_lag{L}\"]  = out[\"speed10\"].shift(L)\n",
    "        out[f\"speed100_lag{L}\"] = out[\"speed100\"].shift(L)\n",
    "\n",
    "    if include_turbulence:\n",
    "        for W in TURB_WINS:\n",
    "            s10  = out[\"speed10\"].shift(1).rolling(W, min_periods=W)\n",
    "            s100 = out[\"speed100\"].shift(1).rolling(W, min_periods=W)\n",
    "            out[f\"speed10_std{W}\"]  = s10.std()\n",
    "            out[f\"speed100_std{W}\"] = s100.std()\n",
    "            out[f\"speed10_rng{W}\"]  = s10.max() - s10.min()\n",
    "            out[f\"speed100_rng{W}\"] = s100.max() - s100.min()\n",
    "            out[f\"y_std{W}\"]        = out[TARGET_COL].shift(1).rolling(W, min_periods=W).std()\n",
    "    return out\n",
    "\n",
    "def build_feat_list(include_turbulence: bool):\n",
    "    base = (\n",
    "        BASE_FEATS +\n",
    "        [\"speed10\",\"speed100\",\"dir10_sin\",\"dir10_cos\",\"dir100_sin\",\"dir100_cos\",\n",
    "         \"shear_speed\",\"veer_sin\",\"veer_cos\",\"hour_sin\",\"hour_cos\",\"day_sin\",\"day_cos\"] +\n",
    "        [f\"y_lag{L}\" for L in LAGS_Y] +\n",
    "        [f\"y_roll{W}\" for W in ROLLS_Y] +\n",
    "        [f\"speed10_lag{L}\" for L in LAGS_SPEED] +\n",
    "        [f\"speed100_lag{L}\" for L in LAGS_SPEED]\n",
    "    )\n",
    "    if not include_turbulence:\n",
    "        return base\n",
    "    turb = (\n",
    "        [f\"speed10_std{W}\"  for W in TURB_WINS] +\n",
    "        [f\"speed100_std{W}\" for W in TURB_WINS] +\n",
    "        [f\"speed10_rng{W}\"  for W in TURB_WINS] +\n",
    "        [f\"speed100_rng{W}\" for W in TURB_WINS] +\n",
    "        [f\"y_std{W}\"        for W in TURB_WINS]\n",
    "    )\n",
    "    return base + turb\n",
    "\n",
    "# ---------- No-class components (same shapes as training) ----------\n",
    "def build_components(input_size, hidden, layers, dropout, bidir):\n",
    "    lstm = nn.LSTM(input_size=input_size,\n",
    "                   hidden_size=hidden,\n",
    "                   num_layers=layers,\n",
    "                   batch_first=True,\n",
    "                   dropout=dropout if layers > 1 else 0.0,\n",
    "                   bidirectional=bidir).to(DEVICE)\n",
    "    out_size = hidden * (2 if bidir else 1)\n",
    "    norm = nn.LayerNorm(out_size).to(DEVICE)\n",
    "    lin1 = nn.Linear(out_size, out_size).to(DEVICE)\n",
    "    lin2 = nn.Linear(out_size, 1).to(DEVICE)\n",
    "    return {\"lstm\": lstm, \"norm\": norm, \"lin1\": lin1, \"lin2\": lin2, \"dropout\": dropout}\n",
    "\n",
    "def forward_pass(x, mods, training: bool):\n",
    "    o, _ = mods[\"lstm\"](x)             # (B,T,H*)\n",
    "    last = o[:, -1, :]\n",
    "    last = mods[\"norm\"](last)\n",
    "    last = F.gelu(mods[\"lin1\"](last))\n",
    "    last = F.dropout(last, p=mods[\"dropout\"], training=training)\n",
    "    yhat = mods[\"lin2\"](last)          # (B,1)\n",
    "    return yhat\n",
    "\n",
    "def set_train_mode(mods, train: bool):\n",
    "    for m in [mods[\"lstm\"], mods[\"norm\"], mods[\"lin1\"], mods[\"lin2\"]]:\n",
    "        m.train(mode=train)\n",
    "\n",
    "# ---------- Artifacts ----------\n",
    "def load_artifacts(model_root: str):\n",
    "    root = Path(model_root)\n",
    "    with open(root / \"..//BiGLSTM//best_params_ga.json\", \"r\") as f:\n",
    "        best_params = json.load(f)\n",
    "    xsc = joblib.load(root / \"../BiGLSTM/x_scaler_ga.pkl\")\n",
    "    ysc = joblib.load(root / \"../BiGLSTM/y_scaler_ga.pkl\")\n",
    "    state_dicts = torch.load(root / \"../BiGLSTM/bilstm_ga_best_noclass.pt\", map_location=DEVICE)\n",
    "    # state_dicts is a dict: {\"lstm\": ..., \"norm\": ..., \"lin1\": ..., \"lin2\": ...}\n",
    "    return best_params, xsc, ysc, state_dicts\n",
    "\n",
    "# ---------- History requirements ----------\n",
    "def max_required_lag(turb_on: bool) -> int:\n",
    "    m = max(max(LAGS_Y), max(ROLLS_Y), max(LAGS_SPEED))\n",
    "    if turb_on:\n",
    "        m = max(m, max(TURB_WINS))\n",
    "    return m  # 24 if no turbulence, else 48\n",
    "\n",
    "def required_history_rows(lookback: int, turb_on: bool) -> int:\n",
    "    return max_required_lag(turb_on) + int(lookback)\n",
    "\n",
    "# ---------- Core helpers ----------\n",
    "def _prepare_window_from_history(df_hist: pd.DataFrame,\n",
    "                                 best_params: dict,\n",
    "                                 xsc, ysc,\n",
    "                                 future_weather: Optional[Dict[str, float]] = None\n",
    "                                 ) -> Tuple[torch.Tensor, pd.Timestamp]:\n",
    "    \"\"\"\n",
    "    Build the model input window (1, lookback, n_feats) for t+1 and return (xb, next_timestamp).\n",
    "    \"\"\"\n",
    "    lookback = int(best_params[\"lookback\"])\n",
    "    turb_on  = bool(best_params[\"turb_on\"])\n",
    "    log_tgt  = bool(best_params[\"log_target\"])\n",
    "\n",
    "    df_hist = df_hist.copy().sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "    # 1) build a placeholder next-hour exogenous row\n",
    "    if future_weather is None:\n",
    "        fut = df_hist.iloc[[-1]][[TIME_COL] + BASE_FEATS].copy()\n",
    "        fut[TIME_COL] = pd.to_datetime(fut[TIME_COL]) + pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        fut = pd.DataFrame([{\n",
    "            TIME_COL: pd.to_datetime(df_hist[TIME_COL].iloc[-1]) + pd.Timedelta(hours=1),\n",
    "            \"U10\": future_weather[\"U10\"], \"V10\": future_weather[\"V10\"],\n",
    "            \"U100\": future_weather[\"U100\"], \"V100\": future_weather[\"V100\"],\n",
    "        }])\n",
    "    fut[TARGET_COL] = np.nan\n",
    "\n",
    "    work = pd.concat([df_hist[[TIME_COL, TARGET_COL] + BASE_FEATS], fut], ignore_index=True)\n",
    "\n",
    "    # 2) FE (lags/rolls/turbulence identical to training)\n",
    "    dfe = add_engineered_features(work, include_turbulence=turb_on)\n",
    "\n",
    "    # 3) We will take the window from the penultimate row's features (history),\n",
    "    #    because the last row is the target t+1 (which has lags computed).\n",
    "    feat_cols = build_feat_list(include_turbulence=turb_on)\n",
    "    dfe_hist = dfe.iloc[:-1].dropna().reset_index(drop=True)\n",
    "    if len(dfe_hist) < lookback:\n",
    "        need = required_history_rows(lookback, turb_on)\n",
    "        raise ValueError(f\"Not enough valid rows after FE to form lookback={lookback} window. \"\n",
    "                         f\"Provide â‰¥ {need} rows of raw history. \"\n",
    "                         f\"Have only {len(df_hist)} pre-FE rows.\")\n",
    "\n",
    "    X_hist = dfe_hist[feat_cols].to_numpy(np.float32)\n",
    "    X_hist_s = xsc.transform(X_hist)\n",
    "\n",
    "    window = X_hist_s[-lookback:, :]  # (lookback, n_feats)\n",
    "    xb = torch.from_numpy(window[None, ...]).float().to(DEVICE)\n",
    "\n",
    "    next_ts = pd.to_datetime(df_hist[TIME_COL].iloc[-1]) + pd.Timedelta(hours=1)\n",
    "    return xb, next_ts, ysc, log_tgt\n",
    "\n",
    "def predict_next_from_history(model_root: str,\n",
    "                              df_history: pd.DataFrame,\n",
    "                              future_weather: Optional[Dict[str, float]] = None) -> Tuple[pd.Timestamp, float]:\n",
    "    \"\"\"\n",
    "    df_history: last N rows with columns [TIMESTAMP, TARGETVAR, U10, V10, U100, V100]\n",
    "    future_weather: optional dict with next-hour exogenous values; if None, holds last known.\n",
    "    \"\"\"\n",
    "    best, xsc, ysc, state = load_artifacts(model_root)\n",
    "\n",
    "    # prepare input window\n",
    "    xb, next_ts, ysc, log_tgt = _prepare_window_from_history(df_history, best, xsc, ysc, future_weather)\n",
    "\n",
    "    # rebuild modules & load weights\n",
    "    input_size = xb.shape[-1]\n",
    "    mods = build_components(input_size, best[\"hidden\"], best[\"layers\"], best[\"dropout\"], best[\"bidir\"])\n",
    "    mods[\"lstm\"].load_state_dict(state[\"lstm\"])\n",
    "    mods[\"norm\"].load_state_dict(state[\"norm\"])\n",
    "    mods[\"lin1\"].load_state_dict(state[\"lin1\"])\n",
    "    mods[\"lin2\"].load_state_dict(state[\"lin2\"])\n",
    "    set_train_mode(mods, False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        yhat_s = forward_pass(xb, mods, training=False).cpu().numpy()  # scaled\n",
    "    yhat = ysc.inverse_transform(yhat_s).ravel()[0]\n",
    "    if log_tgt:\n",
    "        yhat = np.expm1(yhat)\n",
    "    return next_ts, float(yhat)\n",
    "\n",
    "def forecast_multi_steps(model_root: str,\n",
    "                         df_history: pd.DataFrame,\n",
    "                         H: int,\n",
    "                         future_weather_seq: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Recursive forecast for next H hours.\n",
    "    If future_weather_seq is provided, it must have H rows with columns U10,V10,U100,V100 and\n",
    "    an optional TIMESTAMP (used to sanity-check hour steps).\n",
    "    Otherwise, we hold the last known exogenous values.\n",
    "    \"\"\"\n",
    "    best, xsc, ysc, state = load_artifacts(model_root)\n",
    "    lookback = best[\"lookback\"]\n",
    "\n",
    "    # Rebuild modules once\n",
    "    # Prepare initial window from history\n",
    "    xb, next_ts, ysc, log_tgt = _prepare_window_from_history(df_history, best, xsc, ysc, None)\n",
    "    input_size = xb.shape[-1]\n",
    "    mods = build_components(input_size, best[\"hidden\"], best[\"layers\"], best[\"dropout\"], best[\"bidir\"])\n",
    "    mods[\"lstm\"].load_state_dict(state[\"lstm\"])\n",
    "    mods[\"norm\"].load_state_dict(state[\"norm\"])\n",
    "    mods[\"lin1\"].load_state_dict(state[\"lin1\"])\n",
    "    mods[\"lin2\"].load_state_dict(state[\"lin2\"])\n",
    "    set_train_mode(mods, False)\n",
    "\n",
    "    # We'll iterate using the higher-level helper each step so engineered lags/rolls update with predictions\n",
    "    preds, times = [], []\n",
    "    cur_hist = df_history.copy()\n",
    "\n",
    "    for h in range(H):\n",
    "        # choose exogenous for this step if provided\n",
    "        fw = None\n",
    "        if future_weather_seq is not None:\n",
    "            row = future_weather_seq.iloc[h]\n",
    "            fw = {\"U10\": float(row[\"U10\"]), \"V10\": float(row[\"V10\"]),\n",
    "                  \"U100\": float(row[\"U100\"]), \"V100\": float(row[\"V100\"])}\n",
    "\n",
    "        # predict next 1 step\n",
    "        t_next, y_next = predict_next_from_history(model_root, cur_hist, future_weather=fw)\n",
    "        preds.append(y_next); times.append(t_next)\n",
    "\n",
    "        # append to history (so lags/rolls can use the predicted TARGETVAR)\n",
    "        to_append = cur_hist.iloc[[-1]][[TIME_COL] + BASE_FEATS].copy()\n",
    "        to_append[TIME_COL] = t_next\n",
    "        to_append[TARGET_COL] = y_next\n",
    "        if fw is not None:\n",
    "            to_append[\"U10\"] = fw[\"U10\"]; to_append[\"V10\"] = fw[\"V10\"]\n",
    "            to_append[\"U100\"] = fw[\"U100\"]; to_append[\"V100\"] = fw[\"V100\"]\n",
    "        cur_hist = pd.concat([cur_hist, to_append], ignore_index=True)\n",
    "\n",
    "    return pd.DataFrame({TIME_COL: times, \"y_forecast\": preds})\n",
    "\n",
    "def predict_over_file(model_root: str, data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run prediction across an entire file (aligned y_true/y_pred for valid rows).\n",
    "    \"\"\"\n",
    "    best, xsc, ysc, state = load_artifacts(model_root)\n",
    "    turb_on = bool(best[\"turb_on\"])\n",
    "    log_tgt = bool(best[\"log_target\"])\n",
    "    lookback = int(best[\"lookback\"])\n",
    "\n",
    "    # Load\n",
    "    if data_path.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df = pd.read_excel(data_path)\n",
    "    else:\n",
    "        df = pd.read_csv(data_path)\n",
    "    df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\", infer_datetime_format=True)\n",
    "    df = df.sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "    # FE + target transform shape\n",
    "    dfe = add_engineered_features(df, include_turbulence=turb_on).dropna().reset_index(drop=True)\n",
    "    feats = build_feat_list(include_turbulence=turb_on)\n",
    "    X_all = dfe[feats].to_numpy(np.float32)\n",
    "    y_all = dfe[[TARGET_COL]].to_numpy(np.float32)\n",
    "    if log_tgt: y_all = np.log1p(np.clip(y_all, a_min=0, a_max=None))\n",
    "\n",
    "    # scale\n",
    "    Xs = xsc.transform(X_all)\n",
    "    ys = ysc.transform(y_all)\n",
    "\n",
    "    # form sequences as rolling windows over full series\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(lookback, len(Xs)):\n",
    "        X_seq.append(Xs[i - lookback:i, :])\n",
    "        y_seq.append(ys[i, 0])\n",
    "    X_seq = np.array(X_seq, np.float32)\n",
    "    y_seq = np.array(y_seq, np.float32).reshape(-1, 1)\n",
    "\n",
    "    # Rebuild modules & infer in batches\n",
    "    mods = build_components(X_seq.shape[-1], best[\"hidden\"], best[\"layers\"], best[\"dropout\"], best[\"bidir\"])\n",
    "    mods[\"lstm\"].load_state_dict(state[\"lstm\"])\n",
    "    mods[\"norm\"].load_state_dict(state[\"norm\"])\n",
    "    mods[\"lin1\"].load_state_dict(state[\"lin1\"])\n",
    "    mods[\"lin2\"].load_state_dict(state[\"lin2\"])\n",
    "    set_train_mode(mods, False)\n",
    "\n",
    "    preds_s = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_seq), best[\"batch\"]):\n",
    "            xb = torch.from_numpy(X_seq[i:i+best[\"batch\"]]).float().to(DEVICE)\n",
    "            pr = forward_pass(xb, mods, training=False).cpu().numpy()\n",
    "            preds_s.append(pr)\n",
    "    preds_s = np.vstack(preds_s)\n",
    "\n",
    "    # invert target\n",
    "    y_pred = ysc.inverse_transform(preds_s).ravel()\n",
    "    y_true = ysc.inverse_transform(y_seq).ravel()\n",
    "    if log_tgt:\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_true = np.expm1(y_true)\n",
    "\n",
    "    # align timestamps (drop first 'lookback')\n",
    "    out_idx = dfe.index[lookback:]\n",
    "    return pd.DataFrame({\n",
    "        TIME_COL: dfe.loc[out_idx, TIME_COL].values,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762cd290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus X513\\AppData\\Local\\Temp\\ipykernel_16172\\2231355896.py:270: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "df_pred = predict_over_file(model_root=\".\", data_path=r\"E:\\WindPowerForecastingData.xlsx\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Align arrays (if needed)\n",
    "min_len = min(len(df_pred[\"y_true\"]), len(df_pred[\"y_pred\"]))\n",
    "y_true_aligned = np.asarray(df_pred[\"y_true\"]).reshape(-1)[:min_len]\n",
    "y_pred_aligned = np.asarray(df_pred[\"y_pred\"]).reshape(-1)[:min_len]\n",
    "\n",
    "# Save to CSV\n",
    "forecast_df = pd.DataFrame({\n",
    "    \"TIMESTAMP\": df_pred[\"TIMESTAMP\"].values[:min_len],\n",
    "    \"Actual\": y_true_aligned,\n",
    "    \"Predicted\": y_pred_aligned\n",
    "})\n",
    "forecast_df.to_csv(\"forecast_results_biglstm.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27b8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function():\n",
    "    df   = pd.read_csv(\"D:/LSTM_Wind_assignment/data_logs/input_data.csv\")\n",
    "    df_p = pd.read_csv(\"D:/LSTM_Wind_assignment/data_logs/inputs.csv\")\n",
    "\n",
    "    pred_str = str(df_p.loc[0, \"prediction_datetime\"]).strip()\n",
    "    pred_dt  = pd.to_datetime(pred_str, format=\"%Y/%m/%d %H.%M\", errors=\"raise\")\n",
    "\n",
    "    base_str = str(df[\"TIMESTAMP\"].iloc[-1]).strip()\n",
    "    base_dt  = pd.to_datetime(base_str, format=\"%Y%m%d %H:%M\", errors=\"raise\")\n",
    "\n",
    "    time_step = int((pred_dt - base_dt).total_seconds() // 3600)\n",
    "\n",
    "    return time_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
