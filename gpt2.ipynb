{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ae7bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1556/2842457548.py:90: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[TIME_COL] = pd.to_datetime(df[TIME_COL], infer_datetime_format=True, errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data span: 2012-01-01 01:00:00 → 2013-11-30 00:00:00\n",
      "Splits (rows): train=11735 val=2515 test=2515\n",
      "Init 1/12 -> RMSE=0.19571, {'lookback': 6, 'hidden': 32, 'layers': 3, 'dropout': 0.2, 'batch': 32, 'lr': 0.0002395842446100862, 'weight_decay': 0.0}\n",
      "Init 2/12 -> RMSE=0.20622, {'lookback': 48, 'hidden': 32, 'layers': 3, 'dropout': 0.3, 'batch': 32, 'lr': 0.00011237126581726528, 'weight_decay': 1e-05}\n",
      "Init 3/12 -> RMSE=0.24445, {'lookback': 12, 'hidden': 32, 'layers': 3, 'dropout': 0.1, 'batch': 128, 'lr': 0.0012729271343568762, 'weight_decay': 0.001}\n",
      "Init 4/12 -> RMSE=0.18832, {'lookback': 12, 'hidden': 256, 'layers': 3, 'dropout': 0.2, 'batch': 32, 'lr': 0.0019497212456684896, 'weight_decay': 1e-05}\n",
      "Init 5/12 -> RMSE=0.18871, {'lookback': 36, 'hidden': 128, 'layers': 2, 'dropout': 0.1, 'batch': 32, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "Init 6/12 -> RMSE=0.18538, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "Init 7/12 -> RMSE=0.19958, {'lookback': 6, 'hidden': 256, 'layers': 3, 'dropout': 0.0, 'batch': 64, 'lr': 0.000136132035006034, 'weight_decay': 0.0001}\n",
      "Init 8/12 -> RMSE=0.19991, {'lookback': 48, 'hidden': 128, 'layers': 3, 'dropout': 0.1, 'batch': 128, 'lr': 0.00013129365937117703, 'weight_decay': 1e-05}\n",
      "Init 9/12 -> RMSE=0.19346, {'lookback': 24, 'hidden': 32, 'layers': 1, 'dropout': 0.0, 'batch': 64, 'lr': 0.00029686235289069594, 'weight_decay': 0.0001}\n",
      "Init 10/12 -> RMSE=0.18788, {'lookback': 12, 'hidden': 128, 'layers': 2, 'dropout': 0.1, 'batch': 128, 'lr': 0.0002843559352121401, 'weight_decay': 0.0}\n",
      "Init 11/12 -> RMSE=0.19375, {'lookback': 48, 'hidden': 64, 'layers': 3, 'dropout': 0.1, 'batch': 32, 'lr': 0.0006107203059150224, 'weight_decay': 0.0001}\n",
      "Init 12/12 -> RMSE=0.18778, {'lookback': 48, 'hidden': 64, 'layers': 3, 'dropout': 0.2, 'batch': 32, 'lr': 0.00024512167877819755, 'weight_decay': 0.0}\n",
      "\n",
      "Gen 0 best: RMSE=0.18538, params={'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 1] cand -> RMSE=0.18382, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 1] cand -> RMSE=0.18934, {'lookback': 12, 'hidden': 128, 'layers': 2, 'dropout': 0.1, 'batch': 128, 'lr': 0.0002843559352121401, 'weight_decay': 0.0}\n",
      "[Gen 1] cand -> RMSE=0.19557, {'lookback': 48, 'hidden': 64, 'layers': 3, 'dropout': 0.1, 'batch': 32, 'lr': 0.00024512167877819755, 'weight_decay': 0.0}\n",
      "[Gen 1] cand -> RMSE=0.19634, {'lookback': 48, 'hidden': 64, 'layers': 3, 'dropout': 0.2, 'batch': 32, 'lr': 0.0006107203059150224, 'weight_decay': 0.0001}\n",
      "[Gen 1] cand -> RMSE=0.18901, {'lookback': 48, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 1] cand -> RMSE=0.18782, {'lookback': 6, 'hidden': 64, 'layers': 3, 'dropout': 0.2, 'batch': 32, 'lr': 0.00024512167877819755, 'weight_decay': 0.0}\n",
      "[Gen 1] cand -> RMSE=0.19205, {'lookback': 48, 'hidden': 64, 'layers': 3, 'dropout': 0.2, 'batch': 128, 'lr': 0.0002843559352121401, 'weight_decay': 0.0}\n",
      "[Gen 1] cand -> RMSE=0.18761, {'lookback': 12, 'hidden': 128, 'layers': 2, 'dropout': 0.1, 'batch': 32, 'lr': 0.00024512167877819755, 'weight_decay': 0.0}\n",
      "[Gen 1] cand -> RMSE=0.18648, {'lookback': 36, 'hidden': 128, 'layers': 2, 'dropout': 0.1, 'batch': 32, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 1] cand -> RMSE=0.19726, {'lookback': 24, 'hidden': 32, 'layers': 1, 'dropout': 0.0, 'batch': 64, 'lr': 0.00029686235289069594, 'weight_decay': 0.0001}\n",
      "Gen 1 best: RMSE=0.18382, params={'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 2] cand -> RMSE=0.18701, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 2] cand -> RMSE=0.18061, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 2] cand -> RMSE=0.18611, {'lookback': 36, 'hidden': 128, 'layers': 2, 'dropout': 0.1, 'batch': 32, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 2] cand -> RMSE=0.18647, {'lookback': 36, 'hidden': 128, 'layers': 2, 'dropout': 0.1, 'batch': 32, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 2] cand -> RMSE=0.19148, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 2] cand -> RMSE=0.18038, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 2] cand -> RMSE=0.19094, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 32, 'lr': 0.00024512167877819755, 'weight_decay': 0.0}\n",
      "[Gen 2] cand -> RMSE=0.19723, {'lookback': 12, 'hidden': 128, 'layers': 2, 'dropout': 0.1, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 2] cand -> RMSE=0.18383, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 2] cand -> RMSE=0.18449, {'lookback': 36, 'hidden': 128, 'layers': 2, 'dropout': 0.1, 'batch': 32, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "Gen 2 best: RMSE=0.18038, params={'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 3] cand -> RMSE=0.18224, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 3] cand -> RMSE=0.19454, {'lookback': 36, 'hidden': 128, 'layers': 2, 'dropout': 0.1, 'batch': 32, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 3] cand -> RMSE=0.18837, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 3] cand -> RMSE=0.18977, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 3] cand -> RMSE=0.18507, {'lookback': 6, 'hidden': 128, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 3] cand -> RMSE=0.18742, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 3] cand -> RMSE=0.17844, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 3] cand -> RMSE=0.19293, {'lookback': 36, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 3] cand -> RMSE=0.18711, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 3] cand -> RMSE=0.18642, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "Gen 3 best: RMSE=0.17844, params={'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 4] cand -> RMSE=0.18332, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 4] cand -> RMSE=0.18173, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 4] cand -> RMSE=0.18928, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 4] cand -> RMSE=0.18821, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 4] cand -> RMSE=0.18182, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 4] cand -> RMSE=0.18682, {'lookback': 36, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 4] cand -> RMSE=0.17970, {'lookback': 48, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 4] cand -> RMSE=0.18183, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 4] cand -> RMSE=0.18337, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 32, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 4] cand -> RMSE=0.18186, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "Gen 4 best: RMSE=0.17844, params={'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 5] cand -> RMSE=0.18582, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 32, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 5] cand -> RMSE=0.18006, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 5] cand -> RMSE=0.18468, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.002393685353424754, 'weight_decay': 0.0001}\n",
      "[Gen 5] cand -> RMSE=0.18164, {'lookback': 48, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.005011872336272725, 'weight_decay': 0.0001}\n",
      "[Gen 5] cand -> RMSE=0.18870, {'lookback': 36, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 5] cand -> RMSE=0.18251, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 32, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 5] cand -> RMSE=0.18375, {'lookback': 48, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 5] cand -> RMSE=0.18526, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 5] cand -> RMSE=0.18551, {'lookback': 48, 'hidden': 128, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 5] cand -> RMSE=0.18996, {'lookback': 48, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "Gen 5 best: RMSE=0.17844, params={'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 6] cand -> RMSE=0.18479, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.002393685353424754, 'weight_decay': 0.0001}\n",
      "[Gen 6] cand -> RMSE=0.18148, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 6] cand -> RMSE=0.18517, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.002393685353424754, 'weight_decay': 0.0001}\n",
      "[Gen 6] cand -> RMSE=0.18550, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 6] cand -> RMSE=0.18392, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 6] cand -> RMSE=0.18832, {'lookback': 48, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 32, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 6] cand -> RMSE=0.18623, {'lookback': 6, 'hidden': 32, 'layers': 1, 'dropout': 0.0, 'batch': 64, 'lr': 0.005011872336272725, 'weight_decay': 0.0001}\n",
      "[Gen 6] cand -> RMSE=0.18634, {'lookback': 48, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "[Gen 6] cand -> RMSE=0.19403, {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.0027589283556667516, 'weight_decay': 0.0001}\n",
      "[Gen 6] cand -> RMSE=0.18628, {'lookback': 48, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "Gen 6 best: RMSE=0.17844, params={'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "\n",
      "==== GA RESULT ====\n",
      "Best Val RMSE: 0.178442\n",
      "Best Params: {'lookback': 6, 'hidden': 32, 'layers': 2, 'dropout': 0.0, 'batch': 64, 'lr': 0.004238988347579313, 'weight_decay': 0.0001}\n",
      "\n",
      "==== TEST METRICS (retrained on Train+Val) ====\n",
      "RMSE : 0.194692\n",
      "MAE  : 0.151918\n",
      "R^2  : 0.616681\n",
      "sMAPE: 61.92%\n",
      "Saved: lstm_wind_best_ga.pt, feature_scaler_ga.pkl, target_scaler_ga.pkl\n"
     ]
    }
   ],
   "source": [
    "# ga_tune_lstm_wind.py\n",
    "# pip install torch pandas scikit-learn joblib matplotlib\n",
    "\n",
    "import argparse, math, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# ============== CONFIG (edit as you like) ==============\n",
    "TIME_COL     = \"TIMESTAMP\"\n",
    "TARGET_COL   = \"TARGETVAR\"\n",
    "FEATURE_COLS = [\"U10\", \"V10\", \"U100\", \"V100\"]\n",
    "\n",
    "TRAIN_RATIO  = 0.70\n",
    "VAL_RATIO    = 0.15   # TEST = 1 - TRAIN - VAL\n",
    "\n",
    "MAX_EPOCHS   = 40     # per-individual training budget\n",
    "PATIENCE     = 5\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED         = 42\n",
    "\n",
    "# GA search space\n",
    "LOOKBACK_CHOICES   = [6, 12, 24, 36, 48]\n",
    "H_CHOICES          = [32, 64, 128, 256]\n",
    "LAYERS_CHOICES     = [1, 2, 3]\n",
    "DROPOUT_CHOICES    = [0.0, 0.1, 0.2, 0.3, 0.5]\n",
    "BATCH_CHOICES      = [32, 64, 128]\n",
    "LR_LOG10_MIN, LR_LOG10_MAX = -4.0, -2.3   # ~1e-4 to ~5e-3\n",
    "WD_CHOICES         = [0.0, 1e-5, 1e-4, 1e-3]\n",
    "\n",
    "# GA knobs\n",
    "GA_POP       = 12\n",
    "GA_GENS      = 6\n",
    "GA_CX_RATE   = 0.6   # crossover prob\n",
    "GA_MUT_RATE  = 0.25  # mutation prob\n",
    "ELITE_K      = 2     # carry-best each gen\n",
    "\n",
    "# ============== Utilities ==============\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    return 100.0 * np.mean(2.0 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + eps))\n",
    "\n",
    "class SeqDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)      # (B, T, H)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "def make_sequences(X, y, lookback):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(lookback, len(X)):\n",
    "        X_seq.append(X[i-lookback:i, :])\n",
    "        y_seq.append(y[i, 0])\n",
    "    X_seq = np.array(X_seq, dtype=np.float32)\n",
    "    y_seq = np.array(y_seq, dtype=np.float32).reshape(-1, 1)\n",
    "    return X_seq, y_seq\n",
    "\n",
    "# ============== Data prep (once) ==============\n",
    "def load_and_split(file_path, sheet):\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet)\n",
    "    df[TIME_COL] = pd.to_datetime(df[TIME_COL], infer_datetime_format=True, errors=\"coerce\")\n",
    "    df = df.sort_values(TIME_COL).reset_index(drop=True)\n",
    "    data = df[[TIME_COL, TARGET_COL] + FEATURE_COLS].dropna().reset_index(drop=True)\n",
    "\n",
    "    n = len(data)\n",
    "    n_train = int(n * TRAIN_RATIO)\n",
    "    n_val   = int(n * (TRAIN_RATIO + VAL_RATIO))\n",
    "\n",
    "    train_df = data.iloc[:n_train].reset_index(drop=True)\n",
    "    val_df   = data.iloc[n_train:n_val].reset_index(drop=True)\n",
    "    test_df  = data.iloc[n_val:].reset_index(drop=True)\n",
    "\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train = x_scaler.fit_transform(train_df[FEATURE_COLS].values.astype(np.float32))\n",
    "    y_train = y_scaler.fit_transform(train_df[[TARGET_COL]].values.astype(np.float32))\n",
    "    X_val   = x_scaler.transform(val_df[FEATURE_COLS].values.astype(np.float32))\n",
    "    y_val   = y_scaler.transform(val_df[[TARGET_COL]].values.astype(np.float32))\n",
    "    X_test  = x_scaler.transform(test_df[FEATURE_COLS].values.astype(np.float32))\n",
    "    y_test  = y_scaler.transform(test_df[[TARGET_COL]].values.astype(np.float32))\n",
    "\n",
    "    return (train_df, val_df, test_df,\n",
    "            X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "            x_scaler, y_scaler)\n",
    "\n",
    "# ============== Training for a single individual ==============\n",
    "def train_once(Xtr, ytr, Xv, yv, y_scaler, params, max_epochs=MAX_EPOCHS, patience=PATIENCE):\n",
    "    lookback   = params[\"lookback\"]\n",
    "    hidden     = params[\"hidden\"]\n",
    "    layers     = params[\"layers\"]\n",
    "    dropout    = params[\"dropout\"]\n",
    "    batch_size = params[\"batch\"]\n",
    "    lr         = params[\"lr\"]\n",
    "    wd         = params[\"weight_decay\"]\n",
    "\n",
    "    # sequences for this lookback\n",
    "    Xtr_seq, ytr_seq = make_sequences(Xtr, ytr, lookback)\n",
    "    Xv_seq,  yv_seq  = make_sequences(Xv,  yv,  lookback)\n",
    "\n",
    "    # guard (shouldn't happen with our dataset, but be safe)\n",
    "    if len(Xtr_seq) < 10 or len(Xv_seq) < 10:\n",
    "        return float(\"inf\"), None\n",
    "\n",
    "    train_loader = DataLoader(SeqDS(Xtr_seq, ytr_seq), batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_loader   = DataLoader(SeqDS(Xv_seq,  yv_seq ), batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    model = LSTMRegressor(len(FEATURE_COLS), hidden, layers, dropout).to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    bad = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        y_true_s, y_pred_s = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                preds = model(xb).cpu().numpy()\n",
    "                y_pred_s.append(preds)\n",
    "                y_true_s.append(yb.numpy())\n",
    "                losses.append(criterion(model(xb), yb.to(DEVICE)).item())\n",
    "\n",
    "        # compute RMSE in original units for selection\n",
    "        y_pred_s = np.vstack(y_pred_s); y_true_s = np.vstack(y_true_s)\n",
    "        y_pred = y_scaler.inverse_transform(y_pred_s).ravel()\n",
    "        y_true = y_scaler.inverse_transform(y_true_s).ravel()\n",
    "        rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "        if rmse < best_val - 1e-6:\n",
    "            best_val = rmse\n",
    "            bad = 0\n",
    "            best_state = { \"model\": model.state_dict(), \"params\": params.copy() }\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    return best_val, best_state\n",
    "\n",
    "# ============== GA bits ==============\n",
    "def random_individual():\n",
    "    # genome layout: [lookback_idx, hidden_idx, layers_idx, dropout_idx, batch_idx, lr_log10, wd_idx]\n",
    "    return [\n",
    "        random.randrange(len(LOOKBACK_CHOICES)),\n",
    "        random.randrange(len(H_CHOICES)),\n",
    "        random.randrange(len(LAYERS_CHOICES)),\n",
    "        random.randrange(len(DROPOUT_CHOICES)),\n",
    "        random.randrange(len(BATCH_CHOICES)),\n",
    "        random.uniform(LR_LOG10_MIN, LR_LOG10_MAX),\n",
    "        random.randrange(len(WD_CHOICES)),\n",
    "    ]\n",
    "\n",
    "def decode(ind):\n",
    "    return {\n",
    "        \"lookback\": LOOKBACK_CHOICES[ind[0]],\n",
    "        \"hidden\":   H_CHOICES[ind[1]],\n",
    "        \"layers\":   LAYERS_CHOICES[ind[2]],\n",
    "        \"dropout\":  DROPOUT_CHOICES[ind[3]],\n",
    "        \"batch\":    BATCH_CHOICES[ind[4]],\n",
    "        \"lr\":       10.0 ** ind[5],\n",
    "        \"weight_decay\": WD_CHOICES[ind[6]],\n",
    "    }\n",
    "\n",
    "def crossover(p1, p2):\n",
    "    if random.random() > GA_CX_RATE:\n",
    "        return p1[:], p2[:]\n",
    "    cut = random.randint(1, len(p1)-1)\n",
    "    c1 = p1[:cut] + p2[cut:]\n",
    "    c2 = p2[:cut] + p1[cut:]\n",
    "    return c1, c2\n",
    "\n",
    "def mutate(ind):\n",
    "    if random.random() < GA_MUT_RATE:\n",
    "        g = random.randrange(len(ind))\n",
    "        if g in [0,1,2,3,4,6]:  # discrete genes\n",
    "            if g == 0: ind[0] = random.randrange(len(LOOKBACK_CHOICES))\n",
    "            if g == 1: ind[1] = random.randrange(len(H_CHOICES))\n",
    "            if g == 2: ind[2] = random.randrange(len(LAYERS_CHOICES))\n",
    "            if g == 3: ind[3] = random.randrange(len(DROPOUT_CHOICES))\n",
    "            if g == 4: ind[4] = random.randrange(len(BATCH_CHOICES))\n",
    "            if g == 6: ind[6] = random.randrange(len(WD_CHOICES))\n",
    "        else:  # lr_log10 (continuous) -> small Gaussian step\n",
    "            ind[5] += random.gauss(0, 0.2)\n",
    "            ind[5] = max(min(ind[5], LR_LOG10_MAX), LR_LOG10_MIN)\n",
    "    return ind\n",
    "\n",
    "def tournament_select(pop, k=3):\n",
    "    # pop elements are tuples (fitness, ind, state)\n",
    "    cands = random.sample(pop, k)\n",
    "    cands.sort(key=lambda x: x[0])  # lower fitness (RMSE) is better\n",
    "    return cands[0][1][:]  # return a copy of genome\n",
    "\n",
    "# ============== Main ==============\n",
    "def run_ga(file_path, sheet):\n",
    "    set_seed(SEED)\n",
    "    # data (scaled once)\n",
    "    (train_df, val_df, test_df,\n",
    "     Xtr, ytr, Xv, yv, Xte, yte,\n",
    "     x_scaler, y_scaler) = load_and_split(file_path, sheet)\n",
    "\n",
    "    print(f\"Data span: {train_df[TIME_COL].min()} → {test_df[TIME_COL].max()}\")\n",
    "    print(f\"Splits (rows): train={len(train_df)} val={len(val_df)} test={len(test_df)}\")\n",
    "\n",
    "    # init population\n",
    "    pop = []\n",
    "    for i in range(GA_POP):\n",
    "        ind = random_individual()\n",
    "        params = decode(ind)\n",
    "        fit, state = train_once(Xtr, ytr, Xv, yv, y_scaler, params)\n",
    "        pop.append((fit, ind, state))\n",
    "        print(f\"Init {i+1}/{GA_POP} -> RMSE={fit:.5f}, {params}\")\n",
    "\n",
    "    # evolve\n",
    "    best = min(pop, key=lambda x: x[0])\n",
    "    print(f\"\\nGen 0 best: RMSE={best[0]:.5f}, params={decode(best[1])}\")\n",
    "\n",
    "    for gen in range(1, GA_GENS+1):\n",
    "        new_pop = sorted(pop, key=lambda x: x[0])[:ELITE_K]  # elitism\n",
    "\n",
    "        while len(new_pop) < GA_POP:\n",
    "            p1 = tournament_select(pop); p2 = tournament_select(pop)\n",
    "            c1, c2 = crossover(p1, p2)\n",
    "            c1 = mutate(c1); c2 = mutate(c2)\n",
    "\n",
    "            for child in [c1, c2]:\n",
    "                params = decode(child)\n",
    "                fit, state = train_once(Xtr, ytr, Xv, yv, y_scaler, params)\n",
    "                new_pop.append((fit, child, state))\n",
    "                print(f\"[Gen {gen}] cand -> RMSE={fit:.5f}, {params}\")\n",
    "                if len(new_pop) >= GA_POP: break\n",
    "\n",
    "        pop = new_pop\n",
    "        best = min(pop, key=lambda x: x[0])\n",
    "        print(f\"Gen {gen} best: RMSE={best[0]:.5f}, params={decode(best[1])}\")\n",
    "\n",
    "    # best individual\n",
    "    best_fit, best_ind, best_state = min(pop, key=lambda x: x[0])\n",
    "    best_params = decode(best_ind)\n",
    "    print(\"\\n==== GA RESULT ====\")\n",
    "    print(f\"Best Val RMSE: {best_fit:.6f}\")\n",
    "    print(\"Best Params:\", best_params)\n",
    "\n",
    "    # Retrain best on TRAIN+VAL, evaluate on TEST, save artifacts\n",
    "    X_trval = np.vstack([Xtr, Xv])\n",
    "    y_trval = np.vstack([ytr, yv])\n",
    "    lookback = best_params[\"lookback\"]\n",
    "    Xtrv_seq, ytrv_seq = make_sequences(X_trval, y_trval, lookback)\n",
    "    Xte_seq, yte_seq   = make_sequences(Xte,    yte,    lookback)\n",
    "\n",
    "    train_loader = DataLoader(SeqDS(Xtrv_seq, ytrv_seq), batch_size=best_params[\"batch\"], shuffle=True)\n",
    "    test_loader  = DataLoader(SeqDS(Xte_seq,  yte_seq ), batch_size=best_params[\"batch\"], shuffle=False)\n",
    "\n",
    "    model = LSTMRegressor(len(FEATURE_COLS), best_params[\"hidden\"], best_params[\"layers\"], best_params[\"dropout\"]).to(DEVICE)\n",
    "    if best_state and \"model\" in best_state and best_state[\"params\"] == best_params:\n",
    "        # warm-start from the best state found on (Train/Val) if hyperparams identical\n",
    "        model.load_state_dict(best_state[\"model\"])\n",
    "\n",
    "    crit = nn.MSELoss()\n",
    "    opt  = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    bad = 0\n",
    "    # quick early stopping on a small split out of train+val to avoid overfitting\n",
    "    # (alternatively, you can just run fixed epochs since we're on final fit)\n",
    "    for epoch in range(1, MAX_EPOCHS+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(); loss = crit(model(xb), yb); loss.backward(); opt.step()\n",
    "\n",
    "    # Evaluate on TEST (original units)\n",
    "    model.eval()\n",
    "    y_true_s, y_pred_s = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            preds = model(xb.to(DEVICE)).cpu().numpy()\n",
    "            y_pred_s.append(preds)\n",
    "            y_true_s.append(yb.numpy())\n",
    "    y_pred_s = np.vstack(y_pred_s); y_true_s = np.vstack(y_true_s)\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_s).ravel()\n",
    "    y_true = y_scaler.inverse_transform(y_true_s).ravel()\n",
    "\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    smp  = smape(y_true, y_pred)\n",
    "\n",
    "    outdir = Path(\".\")\n",
    "    torch.save(model.state_dict(), outdir / \"lstm_wind_best_ga.pt\")\n",
    "    joblib.dump(x_scaler, outdir / \"feature_scaler_ga.pkl\")\n",
    "    joblib.dump(y_scaler, outdir / \"target_scaler_ga.pkl\")\n",
    "\n",
    "    print(\"\\n==== TEST METRICS (retrained on Train+Val) ====\")\n",
    "    print(f\"RMSE : {rmse:.6f}\")\n",
    "    print(f\"MAE  : {mae:.6f}\")\n",
    "    print(f\"R^2  : {r2:.6f}\")\n",
    "    print(f\"sMAPE: {smp:.2f}%\")\n",
    "    print(\"Saved: lstm_wind_best_ga.pt, feature_scaler_ga.pkl, target_scaler_ga.pkl\")\n",
    "\n",
    "    return best_params\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--file\",  type=str, default=\"WindPowerForecastingData.xlsx\")\n",
    "    p.add_argument(\"--sheet\", type=str, default=\"WindPowerForecastingData\")\n",
    "    return p.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed()\n",
    "#args = parse_args()\n",
    "    a = \"WindPowerForecastingData.xlsx\"\n",
    "    b = \"WindPowerForecastingData\"\n",
    "    run_ga(a, b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
